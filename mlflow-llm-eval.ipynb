{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/03sarath/MLFlow-LLM-Evaluation/blob/main/mlflow-llm-eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoPlrDtLuJdI"
      },
      "source": [
        "#MLflow LLM Evaluation\n",
        "With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as question answering, translating and text summarization. Evaluating LLMs’ performance is slightly different from traditional ML models, as very often there is no single ground truth to compare against. MLflow provides an API ```mlflow.evaluate()``` to help evaluate your LLMs.\n",
        "\n",
        "MLflow’s LLM evaluation functionality consists of 3 main components:\n",
        "\n",
        "1. **A model to evaluate**: it can be an MLflow pyfunc model, a URI pointing to one registered MLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline.\n",
        "\n",
        "2. **Metrics**: the metrics to compute, LLM evaluate will use LLM metrics.\n",
        "\n",
        "3. **Evaluation data**: the data your model is evaluated at, it can be a pandas Dataframe, a python list, a numpy array or an ```mlflow.data.dataset.Dataset()``` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG4vcMsYuXIC"
      },
      "source": [
        "##Quickstart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BV6CslQ35EC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace with your actual key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXR4euJpAZVd"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow pandas datasets transformers anthropic pyngrok tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://6e2f-35-243-144-47.ngrok-free.app\" #Put there your ngrok URI"
      ],
      "metadata": {
        "id": "pWP8HeGiX7IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-VcESMCvyc0"
      },
      "source": [
        "Below is a **simple example** that gives a quick overview of how to evaluate LLMs with MLflow. The example builds a simple question-answering evaluation pipeline using Anthropic's Claude API. It interacts with the Claude API to get responses and then evaluates the responses based on a custom metric. The results, including the average matching accuracy, are logged to MLflow. You can paste it to your IPython or local editor and execute it, and install missing dependencies as prompted. Running the code requires an Anthropic API key, if you don’t have one, you can set it up by following the Anthropic guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJPGSoEJsqX1"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import anthropic\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import string  # Import string module for punctuation removal\n",
        "\n",
        "# 1. Set your Anthropic API key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace!\n",
        "\n",
        "MLFLOW_EXPERIMENT_NAME = \"mean_cal\" # @param {type:\"string\"}\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "# 2. Define your evaluation data\n",
        "eval_data = pd.DataFrame(\n",
        "    {\n",
        "        \"inputs\": [\n",
        "            \"What is MLflow?\",\n",
        "            \"What is Spark?\",\n",
        "        ],\n",
        "        \"ground_truth\": [\n",
        "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
        "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
        "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
        "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
        "            \"machine learning models.\",\n",
        "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
        "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
        "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
        "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
        "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
        "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Define a function to interact with the Claude API\n",
        "def anthropic_model(prompt, system_prompt):\n",
        "    client = anthropic.Anthropic()\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",  # Replace with an available Claude model for you\n",
        "            max_tokens=1024,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        return message.content[0].text\n",
        "    except anthropic.APIStatusError as e:\n",
        "        print(f\"Anthropic API Error: {e}\")\n",
        "        return None\n",
        "    except anthropic.RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded. Waiting and retrying...\")\n",
        "        time.sleep(60)\n",
        "        return anthropic_model(prompt, system_prompt)  # Retry\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. Modify the evaluation loop to use the Claude API\n",
        "with mlflow.start_run() as run:\n",
        "    system_prompt = \"Answer the following question in two sentences\"\n",
        "\n",
        "    # Create a new column with the Claude responses\n",
        "    responses = []\n",
        "    for i in range(len(eval_data)):\n",
        "        response = anthropic_model(eval_data[\"inputs\"][i], system_prompt)\n",
        "        responses.append(response)\n",
        "        print(f\"Claude Response: {response}\") # Print for debugging\n",
        "\n",
        "    eval_data[\"Claude_response\"] = responses\n",
        "\n",
        "    # 5. Evaluate the model (using custom evaluation logic if needed)\n",
        "    def simple_metric(row):\n",
        "        claude_response = row[\"Claude_response\"]\n",
        "        if claude_response is None:  # Handle None values gracefully\n",
        "            return 0.0\n",
        "\n",
        "        ground_truth = row[\"ground_truth\"].lower()\n",
        "        claude_response = claude_response.lower()\n",
        "\n",
        "        # Remove punctuation from both strings\n",
        "        ground_truth = ground_truth.translate(str.maketrans('', '', string.punctuation))\n",
        "        claude_response = claude_response.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        ground_truth_words = ground_truth.split()\n",
        "        claude_words = claude_response.split()\n",
        "\n",
        "        # Check if *any* of the ground truth words are in the Claude response\n",
        "        if any(word in claude_words for word in ground_truth_words):\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    eval_data[\"metric\"] = eval_data.apply(simple_metric, axis=1)  # Apply the metric to each row\n",
        "    average_metric = eval_data[\"metric\"].mean()\n",
        "\n",
        "    # Log the metric to MLflow\n",
        "    mlflow.log_metric(\"simple_matching_accuracy\", average_metric)\n",
        "\n",
        "    print(f\"Simple Matching Accuracy: {average_metric}\")\n",
        "    print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkkRow0zv8cW"
      },
      "source": [
        "##LLM Evaluation Metrics\n",
        "There are two types of LLM evaluation metrics in MLflow:\n",
        "**bold text**\n",
        "1. **Heuristic-based metrics**: These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe), based on certain functions, such as: ```Rouge (rougeL()), Flesch Kincaid (flesch_kincaid_grade_level()) or Bilingual Evaluation Understudy (BLEU) (bleu()).``` These metrics are similar to traditional continuous value metrics. For the list of built-in heuristic metrics and how to define a custom metric with your own function definition, see the Heuristic-based Metrics section.\n",
        "\n",
        "2. **LLM-as-a-Judge metrics**: LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs. It overcomes the limitations of heuristic-based metrics, which often miss nuances like context and semantic accuracy. LLM-as-a-Judge metrics provides a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation. MLflow provides various built-in LLM-as-a-Judge metrics and supports creating custom metrics with your own prompt, grading criteria, and reference examples. See the LLM-as-a-Judge Metrics section for more details.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6SvpyvFweBx"
      },
      "source": [
        "###Heuristic-based Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmrYoIIOtsXz"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate textstat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iLYjS6Mw5PC"
      },
      "source": [
        "MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, “question-answering”. **Depending on the LLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluation**s. **To use defaults metrics for pre-selected tasks, specify the model_type argument in  ```mlflow.evaluate()```, as shown by the example below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtLyFX9gt-bM"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import anthropic\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import string\n",
        "import evaluate  # For toxicity\n",
        "import textstat # For readability\n",
        "\n",
        "# 1. Set your Anthropic API key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace!\n",
        "\n",
        "MLFLOW_EXPERIMENT_NAME = \"toxicity_ari_flesch_scores\" # @param {type:\"string\"}\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "# 2. Define your evaluation data\n",
        "eval_data = pd.DataFrame(\n",
        "    {\n",
        "        \"inputs\": [\n",
        "            \"What is MLflow?\",\n",
        "            \"What is Spark?\",\n",
        "        ],\n",
        "        \"ground_truth\": [\n",
        "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
        "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
        "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
        "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
        "            \"machine learning models.\",\n",
        "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
        "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
        "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
        "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
        "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
        "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Define a function to interact with the Claude API\n",
        "def anthropic_model(prompt, system_prompt):\n",
        "    client = anthropic.Anthropic()\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",  # Replace with an available Claude model for you\n",
        "            max_tokens=1024,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        return message.content[0].text\n",
        "    except anthropic.APIStatusError as e:\n",
        "        print(f\"Anthropic API Error: {e}\")\n",
        "        return None\n",
        "    except anthropic.RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded. Waiting and retrying...\")\n",
        "        time.sleep(60)\n",
        "        return anthropic_model(prompt, system_prompt)  # Retry\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. Initialize the toxicity metric and readability metrics\n",
        "toxicity_metric = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
        "\n",
        "\n",
        "# 5. Modify the evaluation loop to use the Claude API and calculate metrics\n",
        "with mlflow.start_run() as run:\n",
        "    system_prompt = \"Answer the following question in two sentences\"\n",
        "\n",
        "    # Create a new column with the Claude responses\n",
        "    responses = []\n",
        "    for i in range(len(eval_data)):\n",
        "        response = anthropic_model(eval_data[\"inputs\"][i], system_prompt)\n",
        "        responses.append(response)\n",
        "        print(f\"Claude Response: {response}\") # Print for debugging\n",
        "\n",
        "    eval_data[\"Claude_response\"] = responses\n",
        "\n",
        "    # 6. Evaluate the model (using custom evaluation logic and external metrics)\n",
        "    toxicity_scores = []\n",
        "    ari_scores = []\n",
        "    flesch_scores = []\n",
        "\n",
        "    for response in eval_data[\"Claude_response\"]:\n",
        "        if response is None:\n",
        "            toxicity_scores.append(None)\n",
        "            ari_scores.append(None)\n",
        "            flesch_scores.append(None)\n",
        "            continue # Skip evaluation if the response is None\n",
        "\n",
        "        # Calculate toxicity\n",
        "        toxicity_result = toxicity_metric.compute(predictions=[response])\n",
        "        toxicity_score = toxicity_result[\"toxicity\"][0] if toxicity_result[\"toxicity\"] else None  # Handle empty toxicity scores\n",
        "\n",
        "        toxicity_scores.append(toxicity_score)\n",
        "\n",
        "        # Calculate readability scores\n",
        "        ari_score = textstat.automated_readability_index(response)\n",
        "        flesch_score = textstat.flesch_kincaid_grade(response)\n",
        "        ari_scores.append(ari_score)\n",
        "        flesch_scores.append(flesch_score)\n",
        "\n",
        "\n",
        "    eval_data[\"toxicity\"] = toxicity_scores\n",
        "    eval_data[\"ari_grade_level\"] = ari_scores\n",
        "    eval_data[\"flesch_kincaid_grade_level\"] = flesch_scores\n",
        "\n",
        "    # 7. Log metrics to MLflow\n",
        "    # Handle None values when calculating the mean\n",
        "    avg_toxicity = eval_data[\"toxicity\"].dropna().mean() if eval_data[\"toxicity\"].notna().any() else None\n",
        "    avg_ari = eval_data[\"ari_grade_level\"].dropna().mean() if eval_data[\"ari_grade_level\"].notna().any() else None\n",
        "    avg_flesch = eval_data[\"flesch_kincaid_grade_level\"].dropna().mean() if eval_data[\"flesch_kincaid_grade_level\"].notna().any() else None\n",
        "\n",
        "\n",
        "    if avg_toxicity is not None:\n",
        "        mlflow.log_metric(\"avg_toxicity\", avg_toxicity)\n",
        "    if avg_ari is not None:\n",
        "        mlflow.log_metric(\"avg_ari_grade_level\", avg_ari)\n",
        "    if avg_flesch is not None:\n",
        "        mlflow.log_metric(\"avg_flesch_kincaid_grade_level\", avg_flesch)\n",
        "\n",
        "    print(f\"Average Toxicity: {avg_toxicity}\")\n",
        "    print(f\"Average ARI Grade Level: {avg_ari}\")\n",
        "    print(f\"Average Flesch-Kincaid Grade Level: {avg_flesch}\")\n",
        "\n",
        "    print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBkRS4RdxBJG"
      },
      "source": [
        "**Use a Custom List of Metrics**\n",
        "\n",
        "Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics for LLM evaluation in MLflow. You can specify a custom list of metrics in the extra_metrics argument in ```mlflow.evaluate```:\n",
        "\n",
        "To add additional metrics to the default metrics list of pre-defined model type, keep the model_type and add your metrics to extra_metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KWitUaPugkT"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import anthropic\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import string\n",
        "import evaluate  # For toxicity\n",
        "import textstat # For readability\n",
        "import numpy as np\n",
        "\n",
        "# 1. Set your Anthropic API key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace!\n",
        "MLFLOW_EXPERIMENT_NAME = \"toxicity_ari_flesch_latency_scores\" # @param {type:\"string\"}\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "# 2. Define your evaluation data\n",
        "eval_data = pd.DataFrame(\n",
        "    {\n",
        "        \"inputs\": [\n",
        "            \"What is MLflow?\",\n",
        "            \"What is Spark?\",\n",
        "        ],\n",
        "        \"ground_truth\": [\n",
        "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
        "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
        "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
        "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
        "            \"machine learning models.\",\n",
        "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
        "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
        "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
        "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
        "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
        "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Define a function to interact with the Claude API\n",
        "def anthropic_model(prompt, system_prompt):\n",
        "    client = anthropic.Anthropic()\n",
        "    start_time = time.time()  # Capture start time\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",  # Replace with an available Claude model for you\n",
        "            max_tokens=1024,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        response = message.content[0].text\n",
        "        end_time = time.time()  # Capture end time\n",
        "        return response, end_time - start_time # Return response and latency\n",
        "    except anthropic.APIStatusError as e:\n",
        "        print(f\"Anthropic API Error: {e}\")\n",
        "        return None, None  # Return None response and latency\n",
        "    except anthropic.RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded. Waiting and retrying...\")\n",
        "        time.sleep(60)\n",
        "        return anthropic_model(prompt, system_prompt)  # Retry\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# 4. Initialize the toxicity metric and readability metrics\n",
        "toxicity_metric = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
        "\n",
        "# 5. Modify the evaluation loop to use the Claude API and calculate metrics\n",
        "with mlflow.start_run() as run:\n",
        "    system_prompt = \"Answer the following question in two sentences\"\n",
        "\n",
        "    # Create a new column with the Claude responses\n",
        "    responses = []\n",
        "    latencies = []  # List to store latency values\n",
        "    for i in range(len(eval_data)):\n",
        "        response, latency = anthropic_model(eval_data[\"inputs\"][i], system_prompt) #capture two values\n",
        "        responses.append(response)\n",
        "        latencies.append(latency)\n",
        "        print(f\"Claude Response: {response}\")  # Print for debugging\n",
        "        print(f\"Latency: {latency}\")\n",
        "\n",
        "    eval_data[\"Claude_response\"] = responses\n",
        "    eval_data[\"latency\"] = latencies #store latency in df\n",
        "\n",
        "    # 6. Evaluate the model (using custom evaluation logic and external metrics)\n",
        "    toxicity_scores = []\n",
        "    ari_scores = []\n",
        "    flesch_scores = []\n",
        "\n",
        "    for response in eval_data[\"Claude_response\"]:\n",
        "        if response is None:\n",
        "            toxicity_scores.append(None)\n",
        "            ari_scores.append(None)\n",
        "            flesch_scores.append(None)\n",
        "            continue # Skip evaluation if the response is None\n",
        "\n",
        "        # Calculate toxicity\n",
        "        toxicity_result = toxicity_metric.compute(predictions=[response])\n",
        "        toxicity_score = toxicity_result[\"toxicity\"][0] if toxicity_result[\"toxicity\"] else None  # Handle empty toxicity scores\n",
        "\n",
        "        toxicity_scores.append(toxicity_score)\n",
        "\n",
        "        # Calculate readability scores\n",
        "        ari_score = textstat.automated_readability_index(response)\n",
        "        flesch_score = textstat.flesch_kincaid_grade(response)\n",
        "        ari_scores.append(ari_score)\n",
        "        flesch_scores.append(flesch_score)\n",
        "\n",
        "\n",
        "    eval_data[\"toxicity\"] = toxicity_scores\n",
        "    eval_data[\"ari_grade_level\"] = ari_scores\n",
        "    eval_data[\"flesch_kincaid_grade_level\"] = flesch_scores\n",
        "\n",
        "    # 7. Log metrics to MLflow\n",
        "\n",
        "    # Handle None values when calculating the mean\n",
        "    avg_toxicity = eval_data[\"toxicity\"].dropna().mean() if eval_data[\"toxicity\"].notna().any() else None\n",
        "    avg_ari = eval_data[\"ari_grade_level\"].dropna().mean() if eval_data[\"ari_grade_level\"].notna().any() else None\n",
        "    avg_flesch = eval_data[\"flesch_kincaid_grade_level\"].dropna().mean() if eval_data[\"flesch_kincaid_grade_level\"].notna().any() else None\n",
        "    avg_latency = eval_data[\"latency\"].dropna().mean() if eval_data[\"latency\"].notna().any() else None\n",
        "\n",
        "\n",
        "    if avg_toxicity is not None:\n",
        "        mlflow.log_metric(\"avg_toxicity\", avg_toxicity)\n",
        "    if avg_ari is not None:\n",
        "        mlflow.log_metric(\"avg_ari_grade_level\", avg_ari)\n",
        "    if avg_flesch is not None:\n",
        "        mlflow.log_metric(\"avg_flesch_kincaid_grade_level\", avg_flesch)\n",
        "    if avg_latency is not None:\n",
        "        mlflow.log_metric(\"avg_latency\", avg_latency)\n",
        "\n",
        "\n",
        "    print(f\"Average Toxicity: {avg_toxicity}\")\n",
        "    print(f\"Average ARI Grade Level: {avg_ari}\")\n",
        "    print(f\"Average Flesch-Kincaid Grade Level: {avg_flesch}\")\n",
        "    print(f\"Average Latency: {avg_latency}\") # print average latency\n",
        "\n",
        "    print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVO-CYLGxUzl"
      },
      "source": [
        "To disable default metric calculation and only calculate your selected metrics, remove the model_type argument and define the desired metrics.\n",
        "\n",
        "## Calculate Latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1SJ8oY4u63m"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import anthropic\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np  # Import numpy\n",
        "\n",
        "# 1. Set your Anthropic API key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace!\n",
        "MLFLOW_EXPERIMENT_NAME = \"latency_scores\" # @param {type:\"string\"}\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "# 2. Define your evaluation data (you can adjust this as needed)\n",
        "eval_data = pd.DataFrame(\n",
        "    {\n",
        "        \"inputs\": [\n",
        "            \"What is MLflow?\",\n",
        "            \"What is Spark?\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Define a function to interact with the Claude API and measure latency\n",
        "def anthropic_model(prompt, system_prompt):\n",
        "    client = anthropic.Anthropic()\n",
        "    start_time = time.time()  # Capture start time\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",  # Replace with an available Claude model for you\n",
        "            max_tokens=1024,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        end_time = time.time()  # Capture end time\n",
        "        latency = end_time - start_time  # Calculate latency\n",
        "        return latency\n",
        "    except anthropic.APIStatusError as e:\n",
        "        print(f\"Anthropic API Error: {e}\")\n",
        "        return None  # Return None if there's an error\n",
        "    except anthropic.RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded. Waiting and retrying...\")\n",
        "        time.sleep(60)\n",
        "        return anthropic_model(prompt, system_prompt)  # Retry\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. Evaluate latency\n",
        "with mlflow.start_run() as run:\n",
        "    system_prompt = \"Answer the following question in two sentences\" #define the system prompt\n",
        "\n",
        "    latencies = []\n",
        "    for prompt in eval_data[\"inputs\"]:\n",
        "        latency = anthropic_model(prompt, system_prompt)\n",
        "        latencies.append(latency)\n",
        "        print(f\"Prompt: {prompt}, Latency: {latency}\")\n",
        "\n",
        "    eval_data[\"latency\"] = latencies  # Add latency to eval_data DataFrame\n",
        "    # Handle None values when calculating the mean and use only notna values.\n",
        "    avg_latency = eval_data[\"latency\"].dropna().mean() if eval_data[\"latency\"].notna().any() else None\n",
        "\n",
        "    if avg_latency is not None:\n",
        "        mlflow.log_metric(\"avg_latency\", avg_latency)\n",
        "        print(f\"Average Latency: {avg_latency}\")\n",
        "    else:\n",
        "        print(\"No successful API calls to calculate average latency.\")\n",
        "\n",
        "    print(eval_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FvPwfsjAgaq"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY1bhlj_xaz9"
      },
      "source": [
        "###LLM-as-a-Judge Metrics\n",
        "LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs, providing a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation.\n",
        "\n",
        "MLflow supports several builtin LLM-as-a-judge metrics, as well as allowing you to create your own LLM-as-a-judge metrics with custom configurations and prompts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ter_4Zgax_Gm"
      },
      "source": [
        "###Selecting the Judge Model\n",
        "By default, MLflow will use OpenAI’s GPT-4 model as the judge model that scores metrics. You can change the judge model by passing an override to the model argument within the metric definition.\n",
        "\n",
        "1. SaaS LLM Providers\n",
        "To use SaaS LLM providers, such as OpenAI or Anthropic, set the model parameter in the metrics definition, in the format of <provider>:/<model-name>. Currently, MLflow supports [\"openai\", \"anthropic\", \"bedrock\", \"mistral\", \"togetherai\"] as viable LLM providers for any judge model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKagiC6MyIei"
      },
      "source": [
        "Anthropic models can be accessed via the anthropic:/<model-name> URI. Note that the default judge parameters <#overriding-default-judge-parameters> need to be overridden by passing the parameters argument to the metrics definition, since the default parameters violates the Anthropic endpoint requirement (temperature and top_p cannot be specified together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeH2r9CE_eU6"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
        "\n",
        "answer_correctness = mlflow.metrics.genai.answer_correctness(\n",
        "    model=\"anthropic:/claude-3-5-sonnet-20241022\",\n",
        "    # Override default judge parameters to meet Claude endpoint requirements.\n",
        "    parameters={\"temperature\": 0, \"max_tokens\": 256},\n",
        ")\n",
        "\n",
        "# Test the metric definition\n",
        "answer_correctness(\n",
        "    inputs=\"What is MLflow?\",\n",
        "    predictions=\"MLflow is an innovative full self-driving airship.\",\n",
        "    targets=\"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69heOJHRzJCV"
      },
      "source": [
        "###Creating Custom LLM-as-a-Judge Metrics\n",
        "You can also create your own LLM-as-a-Judge evaluation metrics with mlflow.metrics.genai.make_genai_metric() API, which needs the following information:\n",
        "\n",
        "1. name: the name of your custom metric.\n",
        "\n",
        "2. definition: describe what’s the metric doing.\n",
        "\n",
        "3. grading_prompt: describe the scoring criteria.\n",
        "\n",
        "**examples (Optional)**: a few input/output examples with scores provided; used as a reference for the LLM judge.\n",
        "\n",
        "See the API documentation for the full list of the configurations.\n",
        "\n",
        "Under the hood, definition, grading_prompt, examples together with evaluation data and model output will be composed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering, SaaS LLM evaluation metric is basically trying to compose a “right” prompt containing instructions, data and model output so that LLM, e.g., GPT4 can output the information we want.\n",
        "\n",
        "Now let’s create a custom GenAI metrics called “professionalism”, which measures how professional our model output is.\n",
        "\n",
        "Let’s first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples, we will use mlflow.metrics.genai.EvaluationExample() class, which has 4 fields:\n",
        "\n",
        "1. input: input text.\n",
        "\n",
        "2. output: output text.\n",
        "\n",
        "3. score: the score for output in the context of input.\n",
        "\n",
        "4. justification: why do we give the score for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSVK-FqACnb7"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import anthropic\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "from mlflow.metrics import make_metric, MetricValue\n",
        "import re\n",
        "\n",
        "# 1. Set your API keys\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Replace!\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key\" # Replace with google api key!\n",
        "GOOGLE_API_KEY=os.environ[\"GOOGLE_API_KEY\"]\n",
        "\n",
        "MLFLOW_EXPERIMENT_NAME = \"llm-as-a-judge\" # @param {type:\"string\"}\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "# Configure the Google API\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "\n",
        "# 2. Define your models\n",
        "claude_model_id = \"claude-3-opus-20240229\"  # Replace!  Check what models are available\n",
        "google_model_id = \"gemini-1.5-pro-latest\"  # Replace! Check what models are available\n",
        "\n",
        "# 3. Define your evaluation data\n",
        "eval_data = pd.DataFrame(\n",
        "    {\n",
        "        \"inputs\": [\n",
        "            \"What is MLflow?\",\n",
        "            \"What is Spark?\",\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# 4. Set the global evaluation prompt\n",
        "evaluation_prompt = \"\"\"You are an expert evaluator assessing the quality of an answer to a question.\n",
        "\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Assess the answer based on the following criteria:\n",
        "\n",
        "    - Is the answer factually correct and consistent with common knowledge?\n",
        "    - Does the answer address the question completely and accurately?\n",
        "    - Is the answer well-written, clear, and easy to understand?\n",
        "\n",
        "    Provide a score from 1 to 5 and a justification, following the exact format:\n",
        "\n",
        "    Score: [your score]\n",
        "    Justification: [your justification]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 5. All custom functions for the evaluation metrics\n",
        "def get_google_ai_studio_response(prompt, google_model_id):\n",
        "    \"\"\"Calls Google AI Studio and returns the text response or None on error.\"\"\"\n",
        "    model = genai.GenerativeModel(google_model_id)\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Google Generative AI Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_claude_judgment(question, answer, prompt, claude_model_id):\n",
        "    \"\"\"Calls Claude API for evaluation and returns a tuple (score, justification) or (1, \"Error\") on error.\"\"\"\n",
        "    try:\n",
        "        client = anthropic.Anthropic()\n",
        "        message = client.messages.create(\n",
        "            model=claude_model_id,\n",
        "            max_tokens=300,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt.format(question=question, answer=answer)}],\n",
        "        )\n",
        "        claude_response = message.content[0].text\n",
        "\n",
        "        # Use regular expressions to extract the score and justification\n",
        "        match = re.search(r\"Score:\\s*(\\d+)\\s*Justification:\\s*(.+)\", claude_response, re.DOTALL) #Extract the data\n",
        "\n",
        "        if match:\n",
        "            score = int(match.group(1))\n",
        "            justification = match.group(2).strip()\n",
        "            return score, justification\n",
        "        else:\n",
        "            print(f\"Could not parse Claude's response: {claude_response}\")\n",
        "            return 1, \"Could not parse Claude's response.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Claude API: {e}\")\n",
        "        return 1, \"Error during evaluation.\"  # Consistent return\n",
        "\n",
        "\n",
        "def claude_judge_eval_fn(predictions, inputs, claude_model_id):\n",
        "    \"\"\"Evaluates responses using Claude, returns an MLflow MetricValue.\"\"\"\n",
        "    scores = []\n",
        "    justifications = []\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] is None:  # Handle potential None responses\n",
        "            scores.append(1)  # Assign lowest score if Google model returned None\n",
        "            justifications.append(\"Google model returned an error.\")\n",
        "            continue\n",
        "\n",
        "        question = inputs[i]\n",
        "        answer = predictions[i]\n",
        "\n",
        "        score, justification = get_claude_judgment(question=question, answer=answer, prompt=evaluation_prompt, claude_model_id=claude_model_id)\n",
        "        scores.append(score)\n",
        "        justifications.append(justification)\n",
        "\n",
        "    aggregate_results = {\n",
        "        \"mean_score\": sum(scores) / len(scores) if scores else 0,\n",
        "        #Add also the justification\n",
        "    }\n",
        "\n",
        "    # Log detailed results to console\n",
        "    print(\"Detailed Evaluation Results:\")\n",
        "    for i in range(len(inputs)):\n",
        "        print(f\"Question: {inputs[i]}\")\n",
        "        print(f\"Answer: {predictions[i]}\")\n",
        "        print(f\"Score: {scores[i]}\")\n",
        "        print(f\"Justification: {justifications[i]}\")\n",
        "        print(\"---\")\n",
        "\n",
        "\n",
        "    return MetricValue(scores=scores, aggregate_results=aggregate_results)\n",
        "\n",
        "\n",
        "\n",
        "# 6. Define custom evaluation metric\n",
        "claude_judge_metric = make_metric(\n",
        "    eval_fn=claude_judge_eval_fn,\n",
        "    greater_is_better=True,\n",
        "    name=\"claude_answer_correctness\",\n",
        ")\n",
        "\n",
        "# 7. All code connected in the execution\n",
        "with mlflow.start_run() as run:\n",
        "    # Generate Google Model responses\n",
        "    google_responses = []\n",
        "    for prompt in eval_data[\"inputs\"]:\n",
        "        response = get_google_ai_studio_response(prompt, google_model_id)\n",
        "        google_responses.append(response)\n",
        "        print(f\"Google AI Studio Model: {response}\")\n",
        "\n",
        "    # Evaluate using the custom metric\n",
        "    metric_value = claude_judge_metric(predictions=google_responses, inputs=eval_data[\"inputs\"], claude_model_id=claude_model_id)\n",
        "    avg_answer_correctness = metric_value.aggregate_results[\"mean_score\"]\n",
        "\n",
        "    # Log results\n",
        "    mlflow.log_metric(\"avg_answer_correctness\", avg_answer_correctness)\n",
        "\n",
        "    print(f\"Average Answer Correctness from Claude: {avg_answer_correctness}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}